{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying Groq LLM for additional insights...\n",
      "Groq API Error: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 9930, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Groq LLM Response:\n",
      "An error occurred while fetching a response from Groq: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 9930, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Querying Groq LLM for additional insights...\n",
      "Groq API Error: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 9600, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Groq LLM Response:\n",
      "An error occurred while fetching a response from Groq: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 9600, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Querying Groq LLM for additional insights...\n",
      "Groq API Error: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 10022, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Groq LLM Response:\n",
      "An error occurred while fetching a response from Groq: 413, {\"error\":{\"message\":\"Request too large for model `llama-3.3-70b-versatile` in organization `org_01ja0xazkfekmaw6jd0h73jpjn` on tokens per minute (TPM): Limit 6000, Requested 10022, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
      "\n",
      "\n",
      "Querying Groq LLM for additional insights...\n",
      "\n",
      "Groq LLM Response:\n",
      "The text appears to be a collection of slides or notes on various topics related to digital technologies, including digital twins, cloud services, and enterprise resource planning. The key points can be summarized as follows:\n",
      "\n",
      "1. **Digital Twin**: A digital twin is a virtual replica of a physical asset, process, or system that can be used to simulate, analyze, and optimize its behavior. There are different types of digital twins, including component twins, asset twins, system/unit twins, and process twins.\n",
      "2. **Digital Twin Elements**: A digital twin consists of three elements: the physical environment, the virtual environment, and the analytical environment. It uses real-time operational data, CAD models, physics-based models, and statistical models to simulate the behavior of the physical asset or system.\n",
      "3. **Applications of Digital Twins**: Digital twins have various applications across different industries, including automotive, manufacturing, healthcare, energy, and smart cities. They can be used for predictive maintenance, quality control, and optimization of processes.\n",
      "4. **Digital Twin Market**: The digital twin market is growing, with different industries adopting digital twin technology to improve their operations and efficiency.\n",
      "5. **Challenges and Limitations**: The adoption of digital twins is not without challenges, including poor data quality, scalability issues, security and privacy concerns, and organizational resistance to change.\n",
      "6. **Other Digital Technologies**: The text also mentions other digital technologies, including cloud services, enterprise resource planning, collaboration tools, and analytics.\n",
      "7. **Smart Cities and Virtual Spaces**: The concept of digital twins is also applied to smart cities and virtual spaces, where it can be used to simulate and optimize the behavior of complex systems and infrastructure.\n",
      "\n",
      "Overall, the text provides an overview of the concept of digital twins, their applications, and the challenges and limitations associated with their adoption.\n",
      "Summarized text saved to output_ind.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Groq API key from environment variable\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# === Helper Functions === #\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing empty lines.\"\"\"\n",
    "    cleaned_text = \"\\n\".join([line for line in text.splitlines() if line.strip() != \"\"])\n",
    "    return cleaned_text\n",
    "\n",
    "def split_text(text, max_tokens=6000):\n",
    "    \"\"\"Split the text into smaller chunks based on the max token limit.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        word_length = len(word) // 4  # Rough estimate of tokens per word\n",
    "        if current_length + word_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def query_groq_llm(prompt, context, max_tokens=500):\n",
    "    \"\"\"Send a prompt and context to the Groq LLM chat completion API.\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant helping summarize and query text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nContext:\\n{context}\"}\n",
    "            ],\n",
    "            \"model\": \"llama-3.3-70b-versatile\",  # Replace with your desired model\n",
    "            \"max_tokens\": max_tokens,  # Adjust based on your needs\n",
    "            \"temperature\": 0.7  # Adjust randomness\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "        else:\n",
    "            print(f\"Groq API Error: {response.status_code}, {response.text}\")\n",
    "            return f\"An error occurred while fetching a response from Groq: {response.status_code}, {response.text}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Groq API: {e}\")\n",
    "        return f\"An error occurred while fetching a response from Groq: {e}\"\n",
    "\n",
    "# === Main Functionality === #\n",
    "def process_text_file(file_path, output_txt_path):\n",
    "    \"\"\"Process the text file and generate a clean, compacted educational text file.\"\"\"\n",
    "    text = load_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Failed to load text.\")\n",
    "        return\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Split the text into smaller chunks\n",
    "    chunks = split_text(cleaned_text, max_tokens=6000)\n",
    "\n",
    "    # Use a generic query to summarize the entire content\n",
    "    user_query = \"Summarize the key points of the text.\"\n",
    "\n",
    "    summarized_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        # Use Groq API for enhanced summarization\n",
    "        print(\"\\nQuerying Groq LLM for additional insights...\")\n",
    "        llm_response = query_groq_llm(user_query, chunk, max_tokens=500)\n",
    "        print(\"\\nGroq LLM Response:\")\n",
    "        print(llm_response)\n",
    "        summarized_text += llm_response + \"\\n\"\n",
    "\n",
    "    # Save the summarized text to a file\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summarized_text)\n",
    "\n",
    "# === Run the System === #\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"summary_ind.txt\"  # Replace with your text file\n",
    "    output_txt_path = \"output_ind.txt\"\n",
    "    process_text_file(file_path, output_txt_path)\n",
    "    print(f\"Summarized text saved to {output_txt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
