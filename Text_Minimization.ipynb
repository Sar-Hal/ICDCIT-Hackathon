{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "\n",
    "# Set your Groq API key\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# === Helper Functions === #\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_and_split_text(text):\n",
    "    \"\"\"Clean the text and split it into manageable sections.\"\"\"\n",
    "    cleaned_text = text.strip()\n",
    "    sections = cleaned_text.split(\"\\n\\n\")  # Split text into sections by paragraphs\n",
    "    return sections\n",
    "\n",
    "def query_groq_llm(prompt, context):\n",
    "    \"\"\"Send a prompt and context to the Groq LLM chat completion API.\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.groq.com/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant helping summarize and query text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nContext:\\n{context}\"}\n",
    "            ],\n",
    "            \"model\": \"llama-3.3-70b-versatile\"  # Replace with your desired model\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "        else:\n",
    "            print(f\"Groq API Error: {response.status_code}, {response.text}\")\n",
    "            return \"An error occurred while fetching a response from Groq.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Groq API: {e}\")\n",
    "        return \"An error occurred while fetching a response from Groq.\"\n",
    "\n",
    "def build_faiss_index(sections, model):\n",
    "    \"\"\"Build a FAISS index from the text sections.\"\"\"\n",
    "    embeddings = model.encode(sections)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings, dtype=np.float32))\n",
    "    return index, sections\n",
    "\n",
    "def search_faiss(query, model, index, sections, top_k=5):\n",
    "    \"\"\"Search the FAISS index for the most relevant sections.\"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "    results = [{\"section\": sections[i], \"distance\": distances[0][j]} for j, i in enumerate(indices[0])]\n",
    "    return results\n",
    "\n",
    "# === Main Functionality === #\n",
    "def query_text_file(file_path):\n",
    "    \"\"\"Interactive query system for the text file.\"\"\"\n",
    "    text = load_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Failed to load text.\")\n",
    "        return\n",
    "\n",
    "    # Clean and split text\n",
    "    sections = clean_and_split_text(text)\n",
    "\n",
    "    # Load SentenceTransformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Build FAISS index\n",
    "    index, sections = build_faiss_index(sections, model)\n",
    "\n",
    "    print(\"Text query system initialized. Enter a query or type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        user_query = input(\"\\nEnter your query: \").strip()\n",
    "        if user_query.lower() == \"exit\":\n",
    "            print(\"Exiting the query system. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        print(\"\\nSearching for relevant sections...\")\n",
    "        results = search_faiss(user_query, model, index, sections, top_k=3)\n",
    "\n",
    "        if not results:\n",
    "            print(\"No relevant sections found.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\nTop relevant sections:\")\n",
    "        context = \"\"\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"\\nSection {i + 1} (Similarity: {1 / (1 + result['distance']):.4f}):\")\n",
    "            print(result['section'])\n",
    "            context += f\"{result['section']}\\n\"\n",
    "\n",
    "        # Use Groq API for enhanced summarization\n",
    "        print(\"\\nQuerying Groq LLM for additional insights...\")\n",
    "        llm_response = query_groq_llm(user_query, context)\n",
    "        print(\"\\nGroq LLM Response:\")\n",
    "        print(llm_response)\n",
    "\n",
    "# === Run the System === #\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"summary_blockchain.txt\"  # Replace with your text file\n",
    "    query_text_file(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
