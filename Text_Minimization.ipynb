{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading text file: [Errno 2] No such file or directory: 'summary_int.txt'\n",
      "Failed to load text.\n",
      "Summarized text saved to output_ind.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Groq API key from environment variable\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# === Helper Functions === #\n",
    "def load_text(file_path):\n",
    "    \"\"\"Load text from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading text file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing empty lines.\"\"\"\n",
    "    cleaned_text = \"\\n\".join([line for line in text.splitlines() if line.strip() != \"\"])\n",
    "    return cleaned_text\n",
    "\n",
    "def split_text(text, max_tokens=6000):\n",
    "    \"\"\"Split the text into smaller chunks based on the max token limit.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        word_length = len(word) // 4  # Rough estimate of tokens per word\n",
    "        if current_length + word_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def query_groq_llm(prompt, context, max_tokens=500):\n",
    "    \"\"\"Send a prompt and context to the Groq LLM chat completion API.\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant helping summarize and query text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nContext:\\n{context}\"}\n",
    "            ],\n",
    "            \"model\": \"llama-3.3-70b-versatile\",  # Replace with your desired model\n",
    "            \"max_tokens\": max_tokens,  # Adjust based on your needs\n",
    "            \"temperature\": 0.7  # Adjust randomness\n",
    "        }\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "        else:\n",
    "            print(f\"Groq API Error: {response.status_code}, {response.text}\")\n",
    "            return f\"An error occurred while fetching a response from Groq: {response.status_code}, {response.text}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error communicating with Groq API: {e}\")\n",
    "        return f\"An error occurred while fetching a response from Groq: {e}\"\n",
    "\n",
    "# === Main Functionality === #\n",
    "def process_text_file(file_path, output_txt_path):\n",
    "    \"\"\"Process the text file and generate a clean, compacted educational text file.\"\"\"\n",
    "    text = load_text(file_path)\n",
    "    if not text:\n",
    "        print(\"Failed to load text.\")\n",
    "        return\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Split the text into smaller chunks\n",
    "    chunks = split_text(cleaned_text, max_tokens=6000)\n",
    "\n",
    "    # Use a generic query to summarize the entire content\n",
    "    user_query = \"Summarize the key points of the text.\"\n",
    "\n",
    "    summarized_text = \"\"\n",
    "    for chunk in chunks:\n",
    "        # Use Groq API for enhanced summarization\n",
    "        print(\"\\nQuerying Groq LLM for additional insights...\")\n",
    "        llm_response = query_groq_llm(user_query, chunk, max_tokens=500)\n",
    "        print(\"\\nGroq LLM Response:\")\n",
    "        print(llm_response)\n",
    "        summarized_text += llm_response + \"\\n\"\n",
    "\n",
    "    # Save the summarized text to a file\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summarized_text)\n",
    "\n",
    "# === Run the System === #\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"summary_ind.txt\"  # Replace with your text file\n",
    "    output_txt_path = \"output_ind.txt\"\n",
    "    process_text_file(file_path, output_txt_path)\n",
    "    print(f\"Summarized text saved to {output_txt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
