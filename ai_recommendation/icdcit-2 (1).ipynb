{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T05:54:22.342630Z","iopub.execute_input":"2025-01-08T05:54:22.342940Z","iopub.status.idle":"2025-01-08T05:54:22.652868Z","shell.execute_reply.started":"2025-01-08T05:54:22.342913Z","shell.execute_reply":"2025-01-08T05:54:22.652150Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Step 1: Load the data from a JSON file\ndef load_data(file_path):\n    \"\"\"Load the desired skills and job data from a JSON file.\"\"\"\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return data\n\n# Step 2: Create a recommendation system\ndef recommend_courses(data, job_title):\n    \"\"\"Recommend AI courses based on the desired skills for a specific job.\"\"\"\n    # Convert the data to a DataFrame\n    df = pd.DataFrame(data)\n\n    # Combine the 'desired_skills' column into a single string for vectorization\n    df['skills_combined'] = df['desired_skills'].apply(lambda x: ' '.join(x))\n\n    # Use TF-IDF Vectorizer to process the skills data\n    vectorizer = TfidfVectorizer()\n    tfidf_matrix = vectorizer.fit_transform(df['skills_combined'])\n\n    # Find the row corresponding to the given job title\n    job_index = df[df['job_title'] == job_title].index\n    if job_index.empty:\n        return f\"Job title '{job_title}' not found in the dataset.\"\n\n    # Compute cosine similarity\n    similarity_scores = cosine_similarity(tfidf_matrix[job_index[0]], tfidf_matrix)\n\n    # Get the top recommendations (excluding the job itself)\n    similar_jobs_indices = similarity_scores[0].argsort()[-2::-1]\n    recommendations = df.iloc[similar_jobs_indices]\n\n    return recommendations[['job_title', 'desired_skills']]\n\n# Step 3: Example usage\nif __name__ == \"__main__\":\n    # Path to the JSON file\n    json_file_path = \"job_skills_data.json\"\n\n    # Load the data\n    data = load_data(json_file_path)\n\n    # Input: Job title to find course recommendations for\n    input_job_title = input(\"Enter the job title: \")\n\n    # Get recommendations\n    recommended_courses = recommend_courses(data, input_job_title)\n\n    # Display the recommendations\n    print(\"\\nRecommended Courses based on the job title:\")\n    print(recommended_courses)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}